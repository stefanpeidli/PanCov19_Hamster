"""
Author: Stefan Peidli
Aim: Snakemake workflow for scperturb analysis.
Date: 19.02.2024
Run: snakemake
DAG: snakemake --forceall --dag | dot -Tpdf > snake_dag.pdf
Rulegraph: snakemake --forceall --rulegraph | dot -Tpdf > snake_rulegraph.pdf
Description: This workflow produces 'MesAur_timecourse_{subset}.h5', merged 
	Goldhamster data by the following steps:
	- Download GEO data (SRA)
	- Convert SRA to fastq
	- Run cellranger count
	- Merge cellranger count data
	- Annotate with metadata
"""
import yaml
import os
import sys
from pathlib import Path

import pandas as pd
import numpy as np
from snakemake.utils import min_version

### CONFIGURATION ###
min_version("6.4.1")  # set minimum snakemake version
report: "report/workflow.rst"
configfile: "../../../configuration/config.yaml"

runs = list(pd.read_csv('info/SRR_Acc_List_lung.txt', index_col=0).index)  # exported this manually from run selector in GEO

# data directory on scratch
TEMPDIR = Path(config['tmp_path'])
# data directory on work
DIR = Path(config['data_path'])
# path to Goldhamster genome
Gold_gtf = Path(config['Gold_gtf'])

# local utils
sys.path.insert(1, '../../../utils/')
from utils import *

### RULES ###
rule all:
	input:
		expand(DIR / 'MesAur_timecourse_{subset}.h5', subset=['endothelial', 'neutrophils', 'tcells', 'macrophages'])

rule extract_celltype:
	input:
		DIR / 'MesAur_timecourse.h5'
	output:
		DIR / 'MesAur_timecourse_{subset}.h5'
	resources:
		time='01:00:00',
		mem_mb=32000,
		disk_mb=32000
	run:
		import scanpy as sc
		adata = sc.read_h5ad(input[0])

		if wildcards.subset == 'endothelial':
			mask = np.isin(adata.obs.celltype, ['Bronchial', 'Lymphatic', 'Capillary', 'Vein', 'Artery'])
		elif wildcards.subset == 'neutrophils':
			mask = np.isin(adata.obs.celltype, ['Neutrophils', 'mixed2'])
		elif wildcards.subset == 'tcells':
			mask = np.isin(adata.obs.celltype, ['TNKcells'])
		elif wildcards.subset == 'macrophages':
			mask = np.isin(adata.obs.celltype, ['Treml4+Macrophages', 'MonocyticMacrophages', 'InterstitialMacrophages', 'AlveolarMacrophages'])
		else:
			raise ValueError('Unknown subset')
		adata = adata[mask].copy()

		def prepare_embeddings(adata):
			bdata = adata.copy()
			# Seurat-pca based embeddings (aka "integrated")
			sc.pp.neighbors(bdata, use_rep='X_pca_seurat')
			sc.tl.umap(bdata)
			sc.tl.diffmap(bdata)
			bdata.obsm['X_umap_seurat_based'] = bdata.obsm['X_umap']
			bdata.obsm['X_diffmap_seurat_based'] = bdata.obsm['X_diffmap']

			# own embedding without integration ("vanilla")
			sc.pp.highly_variable_genes(bdata, n_top_genes=2000)
			sc.pp.pca(bdata, use_highly_variable=True)
			sc.pp.neighbors(bdata)
			sc.tl.umap(bdata)
			sc.tl.diffmap(bdata)
			bdata.obsm['X_umap_vanilla'] = bdata.obsm['X_umap']
			bdata.obsm['X_diffmap_vanilla'] = bdata.obsm['X_diffmap']
			del bdata.obsm['X_umap']
			del bdata.obsm['X_diffmap']
			return bdata
		
		prepare_embeddings(adata).write(output[0])

rule prepare_and_merge:
	input:
		pca=DIR / 'seurat_exports/seu_lung_new_combined_integrated_annotated_seurat_pca.csv',
		umap=DIR / 'seurat_exports/seu_lung_new_combined_integrated_annotated_seurat_umap.csv',
		metadata=DIR / 'seurat_exports/seu_lung_new_combined_integrated_annotated_seurat_metadata.csv',
		endo_annot=DIR / 'rds_files/ma_seu_endo_seurat_metadata.csv',
		runs_=expand(DIR / "count_data/cellranger_{run}/outs/filtered_feature_bc_matrix.h5", run=runs)
	output:
		DIR / 'MesAur_timecourse_all.h5'
	resources:
		time='2-23:00:00',
		mem_mb='128000',
		disk_mb=128000
	run:
		import scanpy as sc
		import anndata as ad
		import scvelo as scv
		import scrublet as scr

		# From seurat
		pca = pd.read_csv(input['pca'], index_col=0)
		umap = pd.read_csv(input['umap'], index_col=0)
		metadata = pd.read_csv(input['metadata'], index_col=0)

		# ID tables (for knowing which sample is which run)
		run_ids = pd.read_csv('info/run_ids.tsv', sep='\t', index_col=0, header=None)
		SRR_Acc_List_lung = pd.read_csv('info/SRR_Acc_List_lung.txt', index_col=0)
		def get_run(run_):
			print('Loading: ', run_)
			adata = sc.read_h5ad(run_)
			adata.var_names_make_unique()
			sra_name = run_.split('ranger_')[1].split('/velocyto/')[0]
			sampling_timepoint = SRR_Acc_List_lung.loc[sra_name]['sampling_timepoint']
			geo_name = SRR_Acc_List_lung.loc[sra_name]['Sample Name']
			run_id = run_ids.loc[geo_name][1]

			time = sampling_timepoint.replace('h', '').replace('dpi', '')
			addor = 'e' if time == '14' else 'd'
			replicate = run_id.split('#')[-1]

			orig_ident = f'ma_{addor}{time}_lung_{replicate}'

			adata.obs_names = [f"{orig_ident}_{x}" for x in adata.obs_names]
			adata.obsm['X_pca_seurat'] = pd.merge(adata.obs, pca, how='left', left_index=True, right_index=True).values
			adata.obsm['X_umap_seurat'] = pd.merge(adata.obs, umap, how='left', left_index=True, right_index=True).values
			adata.obs = pd.merge(adata.obs, metadata, how='left', left_index=True, right_index=True)

			# Filter like in Seurat
			adata = adata[~pd.isna(adata.obs['UMAP_1'])].copy()
			return adata

		print('Loading data...')
		adatas = [get_run(run_) for run_ in input['runs_']]
		print('Concating adatas...')
		adata = sc.concat(adatas)

		# Add finer endo annotation
		ma_tab = pd.read_csv(input['endo_annot'], index_col=0)
		adata.obs.celltype = adata.obs.celltype.astype(str)
		adata.obs.loc[ma_tab['celltype'].index, 'celltype'] = ma_tab['celltype']

		# Norm and log, we do not scale to keep data sparse
		adata.layers['counts'] = adata.X.copy()
		sc.pp.normalize_per_cell(adata)
		sc.pp.log1p(adata)

		# Annotate doublets
		sample_key = 'orig.ident'
		obs_key = 'celltype'
		parent_cells = infer_doublets_and_parents(adata, sample_key=sample_key, layer='counts')
		tab = count_parent_obs(adata, parent_cells, obs_key=obs_key)
		tab = tab[[k for k in tab.columns if 'mixed' not in k]]  # mixed is not a valid parent
		tab_n = normalize_tab(tab)
		observed, doublets_types = assign_doublets(tab_n, threshold=0.2, symmetric=False)
		add_doublet_type_annotation(adata, doublets_types, tab, obs_key=obs_key, key='doublet_types')
		adata.obs['doublet_types'].to_csv(output[5])

		def prepare_embeddings(adata):
			bdata = adata.copy()
			# Seurat-pca based embeddings (aka "integrated")
			sc.pp.neighbors(bdata, use_rep='X_pca_seurat')
			sc.tl.umap(bdata)
			sc.tl.diffmap(bdata)
			bdata.obsm['X_umap_seurat_based'] = bdata.obsm['X_umap']
			bdata.obsm['X_diffmap_seurat_based'] = bdata.obsm['X_diffmap']

			# own embedding without integration ("vanilla")
			sc.pp.highly_variable_genes(bdata, n_top_genes=2000)
			sc.pp.pca(bdata, use_highly_variable=True)
			sc.pp.neighbors(bdata)
			sc.tl.umap(bdata)
			sc.tl.diffmap(bdata)
			bdata.obsm['X_umap_vanilla'] = bdata.obsm['X_umap']
			bdata.obsm['X_diffmap_vanilla'] = bdata.obsm['X_diffmap']
			del bdata.obsm['X_umap']
			del bdata.obsm['X_diffmap']
			return bdata

		print('Writing total...')
		# Complete set
		prepare_embeddings(adata).write(output[0])

rule R_extract:
	input:
		DIR / 'rds_files/{rds_file}.rds'  # received by first author
	output:
		DIR / 'seurat_exports/{rds_file}_seurat_pca.csv',
		DIR / 'seurat_exports/{rds_file}_seurat_umap.csv',
		DIR / 'seurat_exports/{rds_file}_seurat_metadata.csv'
	resources:
		mem='128G',
		time='02:00:00'
	conda: "seurat_env"
	shell:
		'''
		cd {DIR}
		Rscript --vanilla {UTILS}/extract.R -f rds_files/{wildcards.rds_file}.rds
		mkdir -p seurat_exports/
		mv seurat_pca.csv seurat_exports/{wildcards.rds_file}_seurat_pca.csv
		mv seurat_umap.csv seurat_exports/{wildcards.rds_file}_seurat_umap.csv
		mv seurat_metadata.csv seurat_exports/{wildcards.rds_file}_seurat_metadata.csv
		'''

rule gzip:
	input: "{i}"
	output: "{i}.gz"
	shell: "gzip {input}"

rule cellranger_count:
	input:
		fastqR1=TEMPDIR /  "{run}/FASTQs/{run}_S1_L001_R1_001.fastq",
		fastqR2=TEMPDIR /  "{run}/FASTQs/{run}_S1_L001_R2_001.fastq",
		transcriptome=Gold_gtf
	output:
		cellranger_h5=TEMPDIR / "count_data/cellranger_{run}/outs/filtered_feature_bc_matrix.h5",
		bam=TEMPDIR / "count_data/cellranger_{run}/outs/possorted_genome_bam.bam",
		bai=TEMPDIR / "count_data/cellranger_{run}/outs/possorted_genome_bam.bam.bai"
	threads: 16
	shell:
		'''
		cd {TEMPDIR}/count_data/
		rm -fr cellranger_{wildcards.run}/  # https://twitter.com/johanneskoester/status/1059532816537522176?lang=en
		cellranger count --id=cellranger_{wildcards.run} \
		--fastqs={TEMPDIR}/{wildcards.run}/FASTQs \
		--sample={wildcards.run} \
		--transcriptome={input.transcriptome} \
		--localmem=30 \
		--localcores={threads}
		'''

rule cellranger_mkref:
	input:
		fasta=TEMPDIR / "Genome_MesAur1_0/GCA_000349665.1_MesAur1.0_genomic.fa",
		gtf=TEMPDIR / "GSE162208_gtfs/GSE162208_ma1_genes_longer.gtf"
	output: directory(TEMPDIR / "MesAur1_0_ref")
	log: "log/mkref.log"
	shell:
		'''
		cd {TEMPDIR}
		cat GSE162208_gtfs/GSE162208_ma1_genes_longer.gtf | grep -v "MT\s" > GSE162208_gtfs/GSE162208_ma1_genes_longer_nomito.gtf
		cellranger mkref --genome=MesAur1_0_ref \
		--fasta={input.fasta} \
		--genes=GSE162208_gtfs/GSE162208_ma1_genes_longer_nomito.gtf > {log}
		'''

rule get_genome_annotation:
	output: TEMPDIR / "Genome_MesAur1_0/GCA_000349665.1_MesAur1.0_genomic.fa"
	shell:
		'''
		cd {TEMPDIR}
		mkdir -p Genome_MesAur1_0/
		wget -P Genome_MesAur1_0/ https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/349/665/GCA_000349665.1_MesAur1.0/GCA_000349665.1_MesAur1.0_genomic.fna.gz
		gunzip Genome_MesAur1_0/GCA_000349665.1_MesAur1.0_genomic.fna.gz
		mv Genome_MesAur1_0/GCA_000349665.1_MesAur1.0_genomic.fna Genome_MesAur1_0/GCA_000349665.1_MesAur1.0_genomic.fa
		'''

rule sra_to_fastq:
	input: TEMPDIR / "{run}/{run}.sra"
	output:
		fastqR1=TEMPDIR / "{run}/FASTQs/{run}_S1_L001_R1_001.fastq",
		fastqR2=TEMPDIR / "{run}/FASTQs/{run}_S1_L001_R2_001.fastq"
	shell:
		'''
		mkdir -p {TEMPDIR}/{wildcards.run}/FASTQs/
		fasterq-dump --outdir {TEMPDIR}/{wildcards.run}/FASTQs/ --temp {TEMPDIR}/{wildcards.run}/FASTQs/ {input}
		# Rename for 10x cellranger
		mv {TEMPDIR}/{wildcards.run}/FASTQs/{wildcards.run}_1.fastq {TEMPDIR}/{wildcards.run}/FASTQs/{wildcards.run}_S1_L001_R1_001.fastq
		mv {TEMPDIR}/{wildcards.run}/FASTQs/{wildcards.run}_2.fastq {TEMPDIR}/{wildcards.run}/FASTQs/{wildcards.run}_S1_L001_R2_001.fastq
		'''

rule download_sra:
	output: temp(TEMPDIR / "{run}/{run}.sra")
	shell:
		'''
		prefetch -v {wildcards.run} -O {TEMPDIR}
		'''

rule get_GEO_data:
	output:
		TEMPDIR / "GSE162208_gtfs/GSE162208_ma1_genes_longer.gtf",
		directory(TEMPDIR / "GSE162208_counts")
	shell:
		'''
		cd {TEMPDIR}
		wget --recursive --no-parent -nd -R "index.html*" ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE162nnn/GSE162208/suppl/
		mkdir -p GSE162208_gtfs/
		gunzip GSE162208_ma1_genes_longer.gtf.gz
		mv GSE162208_ma1_genes_longer.gtf GSE162208_gtfs/
		mkdir -p GSE162208_counts/
		tar -xf GSE162208_RAW.tar -C GSE162208_counts/
		rm -f GSE162208_readcounts_ma.tsv.gz
		rm -f pseudobulk.txt
		rm -f filelist.txt
		'''


