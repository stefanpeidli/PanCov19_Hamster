# run as: snakemake --profile=cubi-v1 --jobs 60 -k --use-conda

import pandas as pd
import numpy as np
import os
import sys

runs = pd.read_csv('info/SRR_Acc_List_lung.txt', index_col=0)  # exported this manually from run selector in GEO
runs = list(runs.index)

### PATHS ###
import yaml
with open('../../../config/config.yaml', 'r') as file:
    config = yaml.safe_load(file)
# data directory on scratch
TEMPDIR = config['tmp_path']
# data directory on work
DIR = config['data_path']
Gold_gtf = config['Gold_gtf']
# local utils
sys.path.insert(1, '../../../utils/')
from utils import *

rule all:
	input:
		[DIR+'MesAur_timecourse_'+subset+'.h5' for subset in ['all', 'endothelial', 'neutrophils']]

rule prepare_and_merge:
	input:
		pca=DIR+'seurat_exports/seu_lung_new_combined_integrated_annotated_seurat_pca.csv',
		umap=DIR+'seurat_exports/seu_lung_new_combined_integrated_annotated_seurat_umap.csv',
		metadata=DIR+'seurat_exports/seu_lung_new_combined_integrated_annotated_seurat_metadata.csv',
		endo_annot=DIR+'rds_files/ma_seu_endo_seurat_metadata.csv',
		runs_=expand(DIR+"count_data/cellranger_{run}/outs/filtered_feature_bc_matrix.h5", run=runs)
	output:
		DIR+'MesAur_timecourse_all.h5',
		DIR+'MesAur_timecourse_endothelial.h5',
		DIR+'MesAur_timecourse_neutrophils.h5',
	resources:
		mem='128G',
		time='2-23:00:00',
		disk_mb=128000
	run:
		import scanpy as sc
		import anndata as ad
		import scvelo as scv
		import scrublet as scr
		# from seurat
		pca = pd.read_csv(input['pca'], index_col=0)
		umap = pd.read_csv(input['umap'], index_col=0)
		metadata = pd.read_csv(input['metadata'], index_col=0)

		# ID tables (for knowing which sample is which run)
		run_ids = pd.read_csv('info/run_ids.tsv', sep='\t', index_col=0, header=None)
		SRR_Acc_List_lung = pd.read_csv('info/SRR_Acc_List_lung.txt', index_col=0)
		def get_run(run_):
			print('Loading: ', run_)
			adata = sc.read(run_)
			adata.var_names_make_unique()
			sra_name = run_.split('ranger_')[1].split('/velocyto/')[0]
			sampling_timepoint = SRR_Acc_List_lung.loc[sra_name]['sampling_timepoint']
			geo_name = SRR_Acc_List_lung.loc[sra_name]['Sample Name']
			run_id = run_ids.loc[geo_name][1]

			time = sampling_timepoint.replace('h', '').replace('dpi', '')
			addor = 'e' if time == '14' else 'd'
			replicate = run_id.split('#')[-1]

			orig_ident = 'ma_'+addor+time+'_lung_'+replicate

			adata.obs_names = [orig_ident+'_'+x for x in adata.obs_names]
			adata.obsm['X_pca_seurat'] = pd.merge(adata.obs, pca, how='left', left_index=True, right_index=True).values
			adata.obsm['X_umap_seurat'] = pd.merge(adata.obs, umap, how='left', left_index=True, right_index=True).values
			adata.obs = pd.merge(adata.obs, metadata, how='left', left_index=True, right_index=True)

			# Filter like Emanuel
			adata = adata[~pd.isna(adata.obs['UMAP_1'])].copy()
			return adata

		print('Loading data...')
		adatas = [get_run(run_) for run_ in input['runs_']]
		print('Concating adatas...')
		adata = ad.concat(adatas)

		# add finer endo annotation
		ma_tab = pd.read_csv(input['endo_annot'], index_col=0)
		adata.obs.celltype = adata.obs.celltype.astype(str)
		adata.obs.loc[ma_tab['celltype'].index, 'celltype'] = ma_tab['celltype']

		# norm and log, we do not scale
		adata.layers['counts'] = adata.X.copy()
		sc.pp.normalize_per_cell(adata)
		sc.pp.log1p(adata)

		# doublets
		sample_key = 'orig.ident'
		obs_key = 'celltype'
		parent_cells = infer_doublets_and_parents(adata, sample_key=sample_key, layer='counts')
		tab = count_parent_obs(adata, parent_cells, obs_key=obs_key)
		tab = tab[[k for k in tab.columns if 'mixed' not in k]]  # mixed is not a valid parent
		tab_n = normalize_tab(tab)
		observed, doublets_types = assign_doublets(tab_n, threshold=0.2, symmetric=False)
		add_doublet_type_annotation(adata, doublets_types, tab, obs_key=obs_key, key='doublet_types')
		adata.obs['doublet_types'].to_csv(output[5])

		def prepare_embeddings(adata):
			bdata = adata.copy()
			# Seurat-pca based embeddings (aka "integrated")
			sc.pp.neighbors(bdata, use_rep='X_pca_seurat')
			sc.tl.umap(bdata)
			sc.tl.diffmap(bdata)
			bdata.obsm['X_umap_seurat_based'] = bdata.obsm['X_umap']
			bdata.obsm['X_diffmap_seurat_based'] = bdata.obsm['X_diffmap']

			# own embedding without integration ("vanilla")
			sc.pp.highly_variable_genes(bdata, n_top_genes=2000)
			sc.pp.pca(bdata, use_highly_variable=True)
			sc.pp.neighbors(bdata)
			sc.tl.umap(bdata)
			sc.tl.diffmap(bdata)
			bdata.obsm['X_umap_vanilla'] = bdata.obsm['X_umap']
			bdata.obsm['X_diffmap_vanilla'] = bdata.obsm['X_diffmap']
			del bdata.obsm['X_umap']
			del bdata.obsm['X_diffmap']
			return bdata

		print('Writing total...')
		# Complete set
		prepare_embeddings(adata).write(output[0])

		print('Writing subsets...')
		# Celltype subsets
		prepare_embeddings(adata[np.isin(adata.obs.celltype,
		['Bronchial', 'Lymphatic', 'Capillary', 'Vein', 'Artery'])].copy()).write(output[1])
		prepare_embeddings(adata[np.isin(adata.obs.celltype, ['Neutrophils', 'mixed2'])].copy()).write(output[2])

rule R_extract:
	input:
		DIR+'rds_files/{rds_file}.rds'  # received by first author
	output:
		DIR+'seurat_exports/{rds_file}_seurat_pca.csv',
		DIR+'seurat_exports/{rds_file}_seurat_umap.csv',
		DIR+'seurat_exports/{rds_file}_seurat_metadata.csv'
	resources:
		mem='128G',
		time='02:00:00'
	conda:
		"seurat_env"
	shell:
		'''
		cd {DIR}
		Rscript --vanilla {UTILS}/extract.R -f rds_files/{wildcards.rds_file}.rds
		mkdir -p seurat_exports/
		mv seurat_pca.csv seurat_exports/{wildcards.rds_file}_seurat_pca.csv
		mv seurat_umap.csv seurat_exports/{wildcards.rds_file}_seurat_umap.csv
		mv seurat_metadata.csv seurat_exports/{wildcards.rds_file}_seurat_metadata.csv
		'''

rule gzip:
	input: "{i}"
	output: "{i}.gz"
	shell:
		 "gzip {input}"

rule cellranger_count:
	input:
		fastqR1=TEMPDIR+ "{run}/FASTQs/{run}_S1_L001_R1_001.fastq",
		fastqR2=TEMPDIR+ "{run}/FASTQs/{run}_S1_L001_R2_001.fastq",
		transcriptome=Gold_gtf
	output:
		cellranger_h5=DIR+"count_data/cellranger_{run}/outs/filtered_feature_bc_matrix.h5",
		bam=DIR+"count_data/cellranger_{run}/outs/possorted_genome_bam.bam",
		bai=DIR+"count_data/cellranger_{run}/outs/possorted_genome_bam.bam.bai"
	log: "log/{run}_count.log"
	threads: 12
	shell:
		'''
		cd {DIR}/count_data/
		rm -fr cellranger_{wildcards.run}/  # https://twitter.com/johanneskoester/status/1059532816537522176?lang=en
		cellranger count --id=cellranger_{wildcards.run} \
		--fastqs={TEMPDIR}/{wildcards.run}/FASTQs \
		--sample={wildcards.run} \
		--transcriptome={input.transcriptome} \
		--localmem=30 \
		--localcores=12 > {log}
		'''

rule cellranger_mkref:
	input:
		fasta=DIR+"Genome_MesAur1_0/GCA_000349665.1_MesAur1.0_genomic.fa",
		gtf=DIR+"GSE162208_gtfs/GSE162208_ma1_genes_longer.gtf"
	output: directory(DIR+"MesAur1_0_ref")
	log: "log/mkref.log"
	shell:
		'''
		cd {DIR}
		cat GSE162208_gtfs/GSE162208_ma1_genes_longer.gtf | grep -v "MT\s" > GSE162208_gtfs/GSE162208_ma1_genes_longer_nomito.gtf
		cellranger mkref --genome=MesAur1_0_ref \
		--fasta={input.fasta} \
		--genes=GSE162208_gtfs/GSE162208_ma1_genes_longer_nomito.gtf > {log}
		'''

rule get_genome_annotation:
	output: DIR+"Genome_MesAur1_0/GCA_000349665.1_MesAur1.0_genomic.fa"
	shell:
		'''
		cd {DIR}
		mkdir -p Genome_MesAur1_0/
		wget -P Genome_MesAur1_0/ https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/349/665/GCA_000349665.1_MesAur1.0/GCA_000349665.1_MesAur1.0_genomic.fna.gz
		gunzip Genome_MesAur1_0/GCA_000349665.1_MesAur1.0_genomic.fna.gz
		mv Genome_MesAur1_0/GCA_000349665.1_MesAur1.0_genomic.fna Genome_MesAur1_0/GCA_000349665.1_MesAur1.0_genomic.fa
		'''

rule sra_to_fastq:
	input: TEMPDIR + "{run}/{run}.sra"
	output:
		fastqR1=TEMPDIR + "{run}/FASTQs/{run}_S1_L001_R1_001.fastq",
		fastqR2=TEMPDIR + "{run}/FASTQs/{run}_S1_L001_R2_001.fastq"
	shell:
		'''
		mkdir -p {TEMPDIR}/{wildcards.run}/FASTQs/
		fasterq-dump --outdir {TEMPDIR}/{wildcards.run}/FASTQs/ --temp {TEMPDIR}/{wildcards.run}/FASTQs/ {input}
		# Rename for 10x cellranger
		mv {TEMPDIR}/{wildcards.run}/FASTQs/{wildcards.run}_1.fastq {TEMPDIR}/{wildcards.run}/FASTQs/{wildcards.run}_S1_L001_R1_001.fastq
		mv {TEMPDIR}/{wildcards.run}/FASTQs/{wildcards.run}_2.fastq {TEMPDIR}/{wildcards.run}/FASTQs/{wildcards.run}_S1_L001_R2_001.fastq
		'''

rule download_sra:
	output: temp(TEMPDIR + "{run}/{run}.sra")
	shell:
		'''
		prefetch -v {wildcards.run} -O {DIR}
		'''

rule get_GEO_data:
	output:
		DIR+"GSE162208_gtfs/GSE162208_ma1_genes_longer.gtf",
		directory(DIR+"GSE162208_counts")
	shell:
		'''
		cd {DIR}
		wget --recursive --no-parent -nd -R "index.html*" ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE162nnn/GSE162208/suppl/
		mkdir -p GSE162208_gtfs/
		gunzip GSE162208_ma1_genes_longer.gtf.gz
		mv GSE162208_ma1_genes_longer.gtf GSE162208_gtfs/
		mkdir -p GSE162208_counts/
		tar -xf GSE162208_RAW.tar -C GSE162208_counts/
		rm -f GSE162208_readcounts_ma.tsv.gz
		rm -f pseudobulk.txt
		rm -f filelist.txt
		'''


